import os
import psycopg2
from pgvector.psycopg2 import register_vector
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com" 

# 2. æ•°æ®åº“è¿æ¥é…ç½®
DB_URL = os.getenv("DATABASE_URL")

# 3. åˆå§‹åŒ–å‘é‡æ¨¡å‹
print("æ­£åœ¨åŠ è½½ AI æ¨¡å‹ (ç¬¬ä¸€æ¬¡è¿è¡Œä¼šä¸‹è½½ï¼Œç¨ç­‰)...")
model = SentenceTransformer('BAAI/bge-large-zh-v1.5') 

# --- æ ¸å¿ƒåŠŸèƒ½: å¤„ç†ä¸Šä¼ å¹¶å…¥åº“ (åŸ import_pdf æ”¹åè€Œæ¥) ---
def process_uploaded_file(temp_file_path, original_filename):
    """
    temp_file_path: ç¡¬ç›˜ä¸Šé‚£ä¸ª temp_xxx.pdf çš„è·¯å¾„ (ç”¨æ¥è¯»å–å†…å®¹)
    original_filename: ç”¨æˆ·åŸæœ¬çš„æ–‡ä»¶å (ç”¨æ¥å­˜å…¥æ•°æ®åº“ source å­—æ®µ)
    """
    print(f"ğŸ“˜ [é€»è¾‘å±‚] æ­£åœ¨å¤„ç†æ–‡ä»¶: {original_filename}")
    
    try:
        # --- A. è¯» PDF ---
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¦è¯»çš„æ˜¯ temp_file_path (ä¸´æ—¶æ–‡ä»¶)
        loader = PyPDFLoader(temp_file_path)
        pages = loader.load()
        print(f"   âœ… è¯»åˆ°äº† {len(pages)} é¡µ")

        # --- B. åˆ‡ PDF ---
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=50
        )
        docs = text_splitter.split_documents(pages)
        print(f"   âœ… åˆ‡æˆäº† {len(docs)} ä¸ªè±†è…å—")

        # --- C. è¿æ¥æ•°æ®åº“ ---
        conn = psycopg2.connect(DB_URL)
        
        # ğŸŒŸ å…³é”®è¡¥å……ï¼šæ³¨å†Œ pgvector é€‚é…å™¨
        register_vector(conn) 
        
        cur = conn.cursor()

        print("   ğŸš€ å¼€å§‹å­˜å…¥æ•°æ®åº“...")
        
        for i, doc in enumerate(docs):
            # 1. æ‹¿åˆ°æ–‡å­—å†…å®¹
            content = doc.page_content
            # 2. æ‹¿åˆ°é¡µç 
            page_num = doc.metadata.get('page', 0) + 1
            
            # 3. æ‹¿åˆ°æ–‡ä»¶å (æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¦ç”¨åŸå§‹æ–‡ä»¶åï¼Œè€Œä¸æ˜¯ temp_xxx)
            source_name = original_filename
            
            # 4. å‘é‡åŒ–
            embedding_vector = model.encode(content).tolist()
            
            # 5. æ’å…¥æ•°æ®åº“
            sql = """
                INSERT INTO knowledge_base (content, source, page_number, embedding)
                VALUES (%s, %s, %s, %s);
            """
            cur.execute(sql, (content, source_name, page_num, embedding_vector))
            
            if i % 10 == 0:
                print(f"      å·²å­˜å‚¨ {i}/{len(docs)} å—...", end="\r")

        conn.commit()
        cur.close()
        conn.close()
        
        # âœ… æ”¹åŠ¨ç‚¹ï¼šåŸæ¥æ˜¯ printï¼Œç°åœ¨è¦ return å­—ç¬¦ä¸²ç»™ API
        return f"æˆåŠŸï¼ã€Š{source_name}ã€‹å·²å…¨éƒ¨å­˜å…¥çŸ¥è¯†åº“ï¼Œå…± {len(docs)} æ¡æ•°æ®ã€‚"

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        # æŠŠé”™è¯¯å¾€å¤–æŠ›ï¼Œè®© API çŸ¥é“å‡ºé”™äº†
        raise e

# --- å ä½ç¬¦ï¼šé˜²æ­¢ api.py æŠ¥é”™ ---
def ask_deepseek(question, file_filter=None):
    return "æé—®åŠŸèƒ½ç¨åä¸Šçº¿..."