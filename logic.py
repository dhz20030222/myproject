import os
# 1. é­”æ³•ä»£ç ï¼šå¼ºåˆ¶ä½¿ç”¨å›½å†…é•œåƒ (é˜²æ–­ç½‘)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import psycopg2
from pgvector.psycopg2 import register_vector  # ğŸ‘ˆ å…³é”®ä¿®æ­£1ï¼šå¼•å…¥å‘é‡å·¥å…·
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer

load_dotenv()

# åˆå§‹åŒ– DeepSeek
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"), 
    base_url="https://api.deepseek.com"
)

print("æ­£åœ¨åŠ è½½æœç´¢æ¨¡å‹ (BGE)...")
model = SentenceTransformer('BAAI/bge-large-zh-v1.5')

def ask_deepseek(question_text):
    print(f"\nğŸ“¢ ç”¨æˆ·æé—®: {question_text}")
    
    # --- æ­¥éª¤ A: æœç´¢æ•°æ®åº“ ---
    
    # å…³é”®ä¿®æ­£2ï¼šåŠ ä¸Šæœç´¢å‰ç¼€ï¼Œè®©åŒ¹é…æ›´å‡†
    query_instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
    question_vector = model.encode(query_instruction + question_text).tolist()
    
    try:
        conn = psycopg2.connect(os.getenv("DATABASE_URL"))
        
        # å…³é”®ä¿®æ­£3ï¼šå‘Šè¯‰è¿æ¥å™¨æ€ä¹ˆå¤„ç†å‘é‡
        register_vector(conn)
        
        cur = conn.cursor()
        
        # å…³é”®ä¿®æ­£4ï¼šSQLè¯­å¥åŠ ä¸Š ::vector å¼ºåˆ¶è½¬æ¢
        # æ„æ€æ˜¯ï¼šæŠŠä¼ è¿›æ¥çš„æ•°ç»„(%s)å½“æˆå‘é‡(vector)å»å’Œæ•°æ®åº“é‡Œçš„æ¯”è¾ƒ
        sql = """
            SELECT content, source 
            FROM knowledge_base 
            ORDER BY embedding <=> %s::vector 
            LIMIT 3
        """
        cur.execute(sql, (question_vector,))
        results = cur.fetchall()
        
        cur.close()
        conn.close()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å‡ºé”™: {e}")
        return "æŠ±æ­‰ï¼Œæ•°æ®åº“è¿æ¥å‡ºäº†ç‚¹é—®é¢˜ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚"
    
    # --- æ­¥éª¤ B: ç»„è£…èµ„æ–™ ---
    db_context = ""
    print(f"ğŸ‘€ æ•°æ®åº“æ£€ç´¢ç»“æœ: æ‰¾åˆ°äº† {len(results)} æ¡èµ„æ–™")
    
    if results:
        for i, row in enumerate(results):
            content = row[0]
            source = row[1]
            # æ‰“å°å‡ºæ¥ç»™ä½ çœ‹ï¼Œç¡®è®¤æœ‰æ²¡æœ‰æ‹¿åˆ°â€œSTLâ€é‚£æ®µ
            print(f"   ğŸ“„ [èµ„æ–™{i+1}] {content[:20]}...") 
            db_context += f"--- èµ„æ–™ {i+1} ---\n{content}\n\n"
    else:
        db_context = "æ•°æ®åº“é‡Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

    # --- æ­¥éª¤ C: é—® DeepSeek ---
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è€ƒç ”å¤è¯•åŠ©æ‰‹ã€‚
    è¯·æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ã€ç”¨æˆ·é—®é¢˜ã€‘ã€‚
    
    âš ï¸ è§„åˆ™ï¼š
    1. ç­”æ¡ˆå¿…é¡»åŸºäºå‚è€ƒèµ„æ–™ã€‚
    2. å¦‚æœèµ„æ–™é‡Œæ˜ç¡®æåˆ°äº†ï¼ˆæ¯”å¦‚â€œå…è®¸ä½¿ç”¨STLâ€ï¼‰ï¼Œè¯·ç›´æ¥å‘Šè¯‰ç”¨æˆ·â€œå…è®¸â€ã€‚
    3. å¦‚æœèµ„æ–™é‡ŒçœŸæ²¡æœ‰ï¼Œå°±è¯´ä¸çŸ¥é“ã€‚

    ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
    {db_context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {question_text}
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"DeepSeek æŠ¥é”™å•¦: {str(e)}"

def process_uploaded_file(temp_file_path, filename):
    """
    å¤„ç†ä¸Šä¼ æ–‡ä»¶çš„ç©ºå‡½æ•°ï¼ˆå ä½ç¬¦ï¼‰
    ä¸‹ä¸€æ­¥æˆ‘ä»¬å†æ¥å®ç°å…·ä½“çš„ PDF è¯»å–å’Œå…¥åº“é€»è¾‘
    """
    print(f"ğŸ‘‰ [é€»è¾‘å±‚] æ”¶åˆ°æ–‡ä»¶: {filename}, ä¸´æ—¶è·¯å¾„: {temp_file_path}")
    
    # æš‚æ—¶å…ˆè¿”å›ä¸€ä¸ªå‡ç»“æœï¼Œè¯æ˜æµç¨‹é€šäº†
    return "PDF å¤„ç†åŠŸèƒ½å°šæœªå®ç°ï¼Œä½†æ¥å£è°ƒç”¨æˆåŠŸï¼"