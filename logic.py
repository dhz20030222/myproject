import os
import psycopg2
from pgvector.psycopg2 import register_vector
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer

# 1. åŸºç¡€é…ç½®
load_dotenv()
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
DB_URL = os.getenv("DATABASE_URL")
DEEPSEEK_KEY = os.getenv("DEEPSEEK_API_KEY")

client = OpenAI(api_key=DEEPSEEK_KEY, base_url="https://api.deepseek.com")

# 2. æ¨¡å‹æ‡’åŠ è½½ (è¿™æ˜¯ç»™ upload_handler å€Ÿç”¨çš„æ ¸å¿ƒ)
model = None 

def get_model():
    """æ‡’åŠ è½½ï¼šä¿è¯å…¨å±€åªåŠ è½½ä¸€æ¬¡æ¨¡å‹"""
    global model
    if model is None:
        print("ğŸš€ [ç³»ç»Ÿ] æ­£åœ¨åŠ è½½ embedding æ¨¡å‹ (BGE)...")
        model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
    return model

# --- åŠŸèƒ½ A: è·å–æ–‡ä»¶åˆ—è¡¨ ---
def get_file_list():
    print("ğŸ“‚ [é€»è¾‘å±‚] æ­£åœ¨æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨...")
    try:
        conn = psycopg2.connect(DB_URL)
        cur = conn.cursor()
        cur.execute("SELECT DISTINCT source FROM knowledge_base;")
        files = [row[0] for row in cur.fetchall()]
        cur.close()
        conn.close()
        return files
    except Exception as e:
        print(f"âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return []

# --- åŠŸèƒ½ B: æµå¼é—®ç­” (Generator) ---
def ask_deepseek(question_text, file_filter=None):
    range_info = f"ã€Š{file_filter}ã€‹" if file_filter else "ã€å…¨åº“ã€‘"
    print(f"\nğŸ“¢ [é€»è¾‘å±‚] ç”¨æˆ·æé—®: {question_text} (èŒƒå›´: {range_info})")
    
    # 1. æœç´¢æ•°æ®åº“
    query_instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
    try:
        # è°ƒç”¨è‡ªå·±çš„ get_model()
        question_vector = get_model().encode(query_instruction + question_text).tolist()
        
        conn = psycopg2.connect(DB_URL)
        register_vector(conn)
        cur = conn.cursor()
        
        if file_filter:
            sql = "SELECT content, source FROM knowledge_base WHERE source = %s ORDER BY embedding <=> %s::vector LIMIT 3"
            cur.execute(sql, (file_filter, question_vector))
        else:
            sql = "SELECT content, source FROM knowledge_base ORDER BY embedding <=> %s::vector LIMIT 3"
            cur.execute(sql, (question_vector,))
            
        results = cur.fetchall()
        cur.close()
        conn.close()
    except Exception as e:
        yield f"âŒ æ•°æ®åº“æŠ¥é”™: {e}"
        return

    # 2. ç»„è£… Prompt
    db_context = ""
    if results:
        for row in results:
            db_context += f"--- æ¥æº: {row[1]} ---\n{row[0]}\n\n"
    else:
        db_context = "ï¼ˆæ•°æ®åº“é‡Œæœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼‰"

    prompt = f"è¯·æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\nã€å‚è€ƒèµ„æ–™ã€‘\n{db_context}\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{question_text}"

    # 3. æµå¼è¯·æ±‚ DeepSeek
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„è€ƒç ”åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=True 
        )
        
        for chunk in response:
            content = chunk.choices[0].delta.content
            if content:
                yield content 
    except Exception as e:
        yield f"DeepSeek API æŠ¥é”™: {e}"

# 4. (å¦‚æœä½ å†™äº†è¿™è¡Œ) é¡¶æ ¼å†™çš„å‡½æ•°è°ƒç”¨ï¼šç«‹åˆ»æ‰§è¡Œï¼
# get_model() 